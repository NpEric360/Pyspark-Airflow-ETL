
Note: 
This script is assuming the DAG will be run on a linux machine.
For windows: Airflow will be isntalled using docker instead.
# Installation Steps

1. Install a Ubuntu/Linux machine
    1. WSL
    2. EC2
    3. VM
2. Set up first time use;
    1. Sudo apt update
    2. sudo apt upgrade
3. Install JRE 1.8, JDK 1.8
    1. sudo apt install openjdk-8-jre openjdk-8-jdk
    2. Set path of java installation
        1. Java location path:
            1. readlink -f `which javac` | sed "s:/bin/javac::"
        2. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
        3. source ~/.bashrc
    3. echo $JAVA_HOME to verify path
        1.  java -version
        2. javac -version
4. Python
    1. Sudo apt install python3 python3-pip ipython3
5. Pandas
    1. Pip install pandas
6. Boto3 (AWS SDK)
    1. Pip install boto3
7. Apache Airflow
    1. pip install apache-airflow
    2. Verify using airflow version
        1. ERROR: if airflow isn't being recognized, PATH=$PATH:~/.local/bin
    3. Initialize database
        1. airflow db init
    4. Create user for login
        1. airflow users create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin
    5. Start Airflow Web UI
        1. airflow webserver --port 8080
    6. Start the Airflow Scheduler:
        1. airflow scheduler
    7. To end Airflow process:
        1. Find airflow process id:
            1. find the webservers process id:
            lsof -i tcp:8080 (if port is 8080)
        2. End process
            1. kill <pid>
8. Apache Spark, PySpark
    1. fetch .tgz download link from https://spark.apache.org/downloads.html
    2. wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
    3. ls
    4. Extract
        1. tar -xzvf spark-3.5.0-bin-hadoop3.tgz
    5. Set path:
        1. export SPARK_HOME=/home/nperic360/spark-3.5.0-bin-hadoop3
9. Tableau:
    1. . Setup Tableau and connect to Amazon Athena destination S3 bucket
        1. Download Tableau
        2. Create new
            1. Connect to data â†’ Amazon Athena
        3. You need to download the JDBC driver to connect to Athena
            1. https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html
            2. Extract it and put it into Tableau/Driver
        4. Login creditentials:
            1. SERVER: [athena.us-east-2.amazonaws.com](http://athena.us-east-2.amazonaws.com/)
                1. https://docs.aws.amazon.com/general/latest/gr/athena.html
            2. Athena S3 Destination Bucket:
            3. IAM username and password